{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.output_parsers import JsonOutputToolsParser\n",
    "from langchain_core.tools import tool\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordList(BaseModel):\n",
    "    \"\"\"A list of random words.\n",
    "    \"\"\"\n",
    "    words: List[str] = Field(description=\"A list of words\")\n",
    "    tag: Optional[str] = Field(description=\"A tag for the list of words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_prompt(tag: Optional[str]):\n",
    "    prompt = \"Generate {num_words} random words that are about a {vocab_level} reading level.\"\n",
    "    if tag:\n",
    "        prompt += \"\\n The words should be related to {tag}. Include the tag in the output.\"\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate {num_words} random words that are about a {vocab_level} reading level.\n",
      " The words should be related to {tag}. Include the tag in the output.\n",
      "words=['happy', 'uncomfortable', 'relaxed', 'excited', 'peaceful', 'nervous', 'curious', 'scared', 'content', 'bored'] tag='Feelings in a Place'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import dotenv\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "#from langchain_core.callbacks import CallbackHandler\n",
    "\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "tag = \"how it feels to be in a place\"\n",
    "prompt_str = get_words_prompt(tag)\n",
    "print(prompt_str)\n",
    "word_gen_prompt = PromptTemplate.from_template(\n",
    "    template=prompt_str,\n",
    ")\n",
    "\n",
    "word_gen_model = ChatOpenAI(model=\"gpt-3.5-turbo-1106\", temperature=1, max_tokens=1000)\n",
    "\n",
    "word_gen_model_with_tools = word_gen_model.with_structured_output(WordList)\n",
    "\n",
    "word_gen_chain = word_gen_prompt | word_gen_model_with_tools\n",
    "word_list = word_gen_chain.invoke({\"num_words\": 100, \"vocab_level\": \"third grade\", \"tag\": tag})\n",
    "\n",
    "print(word_list)\n",
    "# Specify the file path where you want to save the JSON file\n",
    "file_path = 'word_list.json'\n",
    "\n",
    "# Load existing data\n",
    "with open(file_path, 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Your new data\n",
    "new_data = word_list.dict()\n",
    "\n",
    "# Append new data\n",
    "data.append(new_data)\n",
    "\n",
    "# Write back to file\n",
    "with open(file_path, 'w') as json_file:\n",
    "    json.dump(data, json_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Specify the file path of the JSON file\n",
    "file_path = 'word_list.json'\n",
    "\n",
    "# Read the JSON file\n",
    "with open(file_path, 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "new_file_path = \"aggregate_words.json\"\n",
    "new_file_data = {}\n",
    "\n",
    "# Iterate through the items\n",
    "for item in data:\n",
    "    # Placeholder for business logic handling each item\n",
    "    # Add your code here to handle each item in the JSON file\n",
    "    tag = item['tag']\n",
    "    for word in item['words']:\n",
    "        if new_file_data.get(word):\n",
    "            new_tags = new_file_data[word]['tags'] + [tag] if tag else new_file_data[word]['tags']\n",
    "            new_tags = list(set(new_tags))\n",
    "            new_file_data[word].update({\n",
    "                \"count\": new_file_data[word]['count'] + 1,\n",
    "                \"tags\": new_tags\n",
    "            })\n",
    "            # print(\"updated a word..\", new_file_data[word])\n",
    "        else:\n",
    "            tags = [tag] if tag else []\n",
    "            new_file_data[word] = {\n",
    "                \"count\": 1,\n",
    "                \"tags\": tags\n",
    "            }\n",
    "            # print(\"added a new word.\", new_file_data[word])\n",
    "\n",
    "# Write the new file data to the JSON file\n",
    "with open(new_file_path, 'w') as json_file:\n",
    "    json.dump(new_file_data, json_file, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "724\n"
     ]
    }
   ],
   "source": [
    "print(len(new_file_data.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-outter-monologue",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
